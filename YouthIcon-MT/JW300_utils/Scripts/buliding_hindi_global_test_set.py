# -*- coding: utf-8 -*-
"""buliding_hindi_global_test_set.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1loxP5XxcSTbOwDjd7RVwvRHcB78Vc0CW

# Setting up the Data
Our first task is to setting up data. We will download the Hindi global test set. After this we need to set english and hindi as source and target language, then we find the intersection of the english test set with the english corpus after that we get the corresponding hindi sentencs from the hindi corpus.

## Mounting the Google Drive
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
source_language = "en"
target_language = "hi" # hi is the language code of hindi
lc = False  # If True, lowercase the data.
seed = 42  # Random  for shuffling.
tag = "baseline" # Give a unique name to your folder - this will ensure you haven't rewritten any models you've already submitted

os.environ["src"] = source_language # Sets them in bash as well, since we often use bash scripts
os.environ["tgt"] = target_language
os.environ["tag"] = tag

# This will save it to a folder in our gdrive instead!
!mkdir -p "/content/drive/My Drive/youthicon/$src-$tgt-$tag"
os.environ["gdrive_path"] = "/content/drive/My Drive/youthicon/%s-%s-%s" % (source_language, target_language, tag)

!echo $gdrive_path

"""# **Downloading the corpus data**

for precaution , am removing the old data
"""

!rm -f w300.$src jw300.$tgt JW300_latest_xml_$src-$tgt.xml.gz JW300_latest_xml_$src-$tgt.xml JW300_latest_xml_$src.zip  JW300_latest_xml_$tgt.zip

pip install opustools-pkg

# Downloading our corpus
! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q

# extract the corpus file
! gunzip JW300_latest_xml_$src-$tgt.xml.gz

! wget https://github.com/osDFS/YouthIcon/raw/master/YouthIcon-MT/JW300_utils/test/test.en-any.en
  
# And the specific test set for this language pair.
os.environ["trg"] = target_language 
os.environ["src"] = source_language

# Read the test data to filter from train and dev splits.
# Store english portion in set for quick filtering checks.
en_test_sents = set()
filter_test_sents = "test.en-any.en"
j = 0
with open(filter_test_sents) as f:
  for line in f:
    en_test_sents.add(line.strip())
    j += 1
print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))

!ls

"""**Building the corpus**

For those who knows hindi , in the 2 cells bellows am checking if the 2 dataset are aligned
"""

! head -5 jw300.en

! head -5 jw300.hi

import pandas as pd

# TMX file to dataframe
source_file = 'jw300.' + source_language  ## source language is english
target_file = 'jw300.' + target_language ## Target is hindi
hindi_test = {}
source = []
target = []
english_sentences_in_global_test_set = {}  # Collect the line numbers of the source portion to skip the same lines for the target portion.
with open(source_file) as src_f:
    for i, line in enumerate(src_f):
        # Skip sentences that are contained in the test set and add them into the new hindi test
        if line.strip() not in en_test_sents:
            source.append(line.strip())
        else:
            # TODOS : Here is the intersection with the global test set
            english_sentences_in_global_test_set[i] = line.strip()           
with open(target_file) as f:
    for j, line in enumerate(f):
        # Only add to corpus if corresponding source was not skipped.
        if j not in english_sentences_in_global_test_set.keys():
            target.append(line.strip())
        else:
            #TODOS : Collecting the aligned test sentences
            hindi_test[j] = line.strip()
    
print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(english_sentences_in_global_test_set.keys()), i))
    
df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])
# if you get TypeError: data argument can't be an iterator is because of your zip version run this below
#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])
df.head(10)

# It will tell us what keys it has then we will enter the key number
hindi_test.keys()

hindi_test[8365]

english_sentences_in_global_test_set[8365]

hindi_test_set = pd.DataFrame(zip(hindi_test.values(), english_sentences_in_global_test_set.values()), columns=['hindi_equivalent', 'english_equivalent'])

hindi_test_set = hindi_test_set.reset_index()

hindi_test_set = hindi_test_set.set_index("index")

hindi_test_set.tail()

hindi_test_set = hindi_test_set.drop_duplicates(subset='hindi_equivalent')

hindi_test_set = hindi_test_set.drop_duplicates(subset='english_equivalent')

hindi_test_set.head()

hindi_test_set.shape

hindi_test_set.loc[~hindi_test_set.english_equivalent.isin(en_test_sents)]

with open("test.hi-any.hi", "w") as test_hi_any_hi:
    test_hi_any_hi.write("\n".join(hindi_test_set.hindi_equivalent))

!head -5 test.hi-any.hi

