{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ger = spacy.load(\"de\")  #loads german language dataset\n",
    "spacy_eng = spacy.load(\"en\")   #loads english language dataset\n",
    "\n",
    "# You can use this too to load dataset if they are linked after downloading. \n",
    "\n",
    "# spacy_ger = spacy.load(\"de\")  #loads german language dataset\n",
    "# spacy_eng = spacy.load(\"en\")   #loads english language dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking the sentence and saving it in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nExample ,\\nSuppose this is an English sentence: \\n'Hello, my name is  magician',\\nthen tokenizer function will seperate each word from sentence, adn then save it in form of list of strings\\n=> ['Hello', 'my', 'name', 'is' , 'magician']\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_ger(text):               #takes an sentence as input.\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "'''\n",
    "Example ,\n",
    "Suppose this is a German sentence: \n",
    "'Hello, my name is  magician',\n",
    "then tokenizer function will seperate each word from sentence, adn then save it in form of list of strings\n",
    "=> ['Hello', 'my', 'name', 'is' , 'magician']\n",
    "\n",
    "''' \n",
    "\n",
    "\n",
    "def tokenize_eng(text):               #takes an sentence as input.\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "# same as tokenize_ger function\n",
    "'''\n",
    "Example ,\n",
    "Suppose this is an English sentence: \n",
    "'Hello, my name is  magician',\n",
    "then tokenizer function will seperate each word from sentence, adn then save it in form of list of strings\n",
    "=> ['Hello', 'my', 'name', 'is' , 'magician']\n",
    "\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How pre-processing of text/ data is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How pre-processing of text/ data is done:\n",
    "\n",
    "german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "\n",
    "# Field function use list the tokenizer function made, Lower for making every letter lowercase,\n",
    "#                    init_token is for specifing start of sentence(<sos>) , eos_token for end of sentence (<eos>)\n",
    "\n",
    "english = Field(\n",
    "    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
    ")\n",
    "# Field function use list the tokenizer function made, Lower for making every letter lowercase,\n",
    "#                    init_token is for specifing start of sentence(<sos>) , eos_token for end of sentence (<eos>)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"), fields=(german, english)\n",
    ")\n",
    "\n",
    "## Spliting data with extension .de for german, .en for english, in field\n",
    "##               same order as for exts, use for calling Field functions for each language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary for each language: \n",
    "\n",
    "# For german vocabulary:\n",
    "\n",
    "german.build_vocab(train_data, max_size=10000, min_freq=2)  \n",
    "#               min_freq = 2 is used so that, word should have to be repeated atleast 2 times,\n",
    "#                           then only it will be added to vocabulary\n",
    "\n",
    "# For English Vocabulary:\n",
    "\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2) \n",
    "#               min_freq = 2 is used so that, word should have to be repeated atleast 2 times,\n",
    "#                            then only it will be added to vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first LSTM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "                       # input_size= size of vocab.(for german lang) ,\n",
    "                       #   embedding_size so that each word is mapped to some D-space\n",
    "                       #          num_layer= no. of layers in Encoder\n",
    "        super(Encoder, self).__init__()\n",
    "                  #  The super() function in Python makes class inheritance more manageable and extensible. The function returns a temporary object that allows reference to a parent class by the keyword super.\n",
    "                  # The super() function has two major use cases:\n",
    "                  #  To avoid the usage of the super (parent) class explicitly.\n",
    "                  #  To enable multiple inheritancesâ€‹.    \n",
    "        self.dropout = nn.Dropout(p)    # p= how many nodes to drop.\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)    # Embedding of some input_size mapped to embedding_size\n",
    "\n",
    "                       # first we run input on embedding, then we run RNN on those embeddings\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)  \n",
    "                            #  input of LSTM is embedding_size, ouyputting hidden_size, dropout is key argument.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # sending an input of a long vector of the indexes of words in vocab through tokenize func()\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size) \n",
    "        #      N is shape of X, so that each word is mapped to embedding , by adding embedding size, we added another Dimension to our tensor.\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "            # imp. to us are hidden and cell.\n",
    "        return hidden, cell\n",
    "\n",
    "# Second LSTM\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        \n",
    "         # input_size= size of vocab.(for English lang) ,\n",
    "         #   embedding_size so that each word is mapped to some D-space\n",
    "         #   num_layer= no. of layers in Decoder\n",
    "         # output_size =  same as input_size, but gives value of vector for each word in vocab.\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)     # p= how many nodes to drop.\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)    # Embedding of some input_size mapped to embedding_size\n",
    "            # first we run input on embedding, then we run RNN on those embeddings\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "                 #  input of LSTM is embedding_size, ouyputting hidden_size, dropout is key argument.\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "                 #fc= fully connected : linear for hiden_size, output_size\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "            # sending an input of a long vector of the indexes of words in vocab through tokenize func()\n",
    "\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0)  #adding 1 D.\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "# for combining encoder and decoder \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "                            # teacher_force_ratio  is used so that model will be fed with\n",
    "                            # 50% correct sentence/words and 50 incorrect.\n",
    "                            # so that it doesn't create problem when we fed test data to machine.\n",
    "                            \n",
    "        batch_size = source.shape[1]  #second dimension of source shape\n",
    "        \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "                # Predict one word at a time, and for each word we predict\n",
    "                # we gonna do it for entire batch , and then every prediction will\n",
    "                # be sort of that vector of entire vocab size.\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're ready to define everything we need for training our Seq2Seq model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")\n",
    "            # lambda func() in sort_key is used to prioritize examples of similar lenght\n",
    "            # so that to minimise number of paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all variables sent in function are declared in cell above name hyperparameter\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "\n",
    "sentence = \"ein boot mit mehreren mÃ¤nnern darauf wird von einem groÃŸen pferdegespann ans ufer gezogen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing functions from other .ipynb file for operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils2.ipynb\n"
     ]
    }
   ],
   "source": [
    "# if you got problem importing other ipynb file from same directory, use\n",
    "# pip install import_ipynb\n",
    "import import_ipynb\n",
    "from utils2 import translate_sentence, bleu, save_checkpoint, load_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['bath', 'bath', 'content', 'several', 'barricades', 'barricades', 'cadillac', 'guidebook', 'guidebook', 'liquid', 'liquid', 'liquid', 'hairnet', 'directions', 'directions', 'soup', 'chiseling', 'breakfast', 'breakfast', 'follow', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy', 'heavy']\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "            # Get input and targets and get to cuda\n",
    "            inp_data = batch.src.to(device)\n",
    "            target = batch.trg.to(device)\n",
    "\n",
    "            # Forward prop\n",
    "            output = model(inp_data, target)\n",
    "\n",
    "            # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "            # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "            # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "            # way that we have output_words * batch_size that we want to send in into\n",
    "            # our cost function, so we need to do some reshapin. While we're at it\n",
    "            # Let's also remove the start token while we're at it\n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            target = target[1:].reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Back prop\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "            # within a healthy range\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "            # Gradient descent step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Plot to tensorboard\n",
    "            writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "            step += 100\n",
    "\n",
    "\n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
