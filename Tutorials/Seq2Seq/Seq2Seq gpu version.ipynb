{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k          \n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /opt/conda/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (46.1.3.post20200325)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.45.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/de_core_news_sm -->\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "spacy_ger = spacy.load('de')  #loads german language dataset\n",
    "spacy_eng = spacy.load('en')   #loads english language dataset\n",
    "\n",
    "# You can use this too to load dataset if they are linked after downloading. \n",
    "\n",
    "# to download german language medium size dataset, use\n",
    "# !python -m spacy download de_core_news_md\n",
    "# for largets dataset\n",
    "# !python -m spacy download de_core_news_ld\n",
    "\n",
    "# to load these dataset, use\n",
    "# spacy_ger = spacy.load(\"de_core_news_md\")  #loads german language dataset of medium size\n",
    "# spacy_ger = spacy.load(\"de_core_news_lg\")  #loads german language dataset of larger size\n",
    "\n",
    "# to download english language medium size dataset, use\n",
    "# !python -m spacy download en_core_web_md\n",
    "# for largets dataset\n",
    "# !python -m spacy download en_core_web_ld\n",
    "\n",
    "# to load these dataset, use\n",
    "# spacy_eng = spacy.load(\"en_core_web_md\")   #loads english language dataset of medium size\n",
    "# spacy_eng = spacy.load(\"en_core_web_lg\")   #loads english language dataset of larger size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking the sentence and saving it in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nExample ,\\nSuppose this is an English sentence: \\n'Hello, my name is  magician',\\nthen tokenizer function will seperate each word from sentence, adn then save it in form of list of strings\\n=> ['Hello', 'my', 'name', 'is' , 'magician']\\n\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_ger(text):               #takes an sentence as input.\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "'''\n",
    "Example ,\n",
    "Suppose this is a German sentence: \n",
    "'Hello, my name is  magician',\n",
    "then tokenizer function will seperate each word from sentence, and then save it in form of list of strings\n",
    "=> ['Hello', 'my', 'name', 'is' , 'magician']\n",
    "\n",
    "''' \n",
    "\n",
    "\n",
    "def tokenize_eng(text):               #takes an sentence as input.\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "# same as tokenize_ger function\n",
    "'''\n",
    "Example ,\n",
    "Suppose this is an English sentence: \n",
    "'Hello, my name is  magician',\n",
    "then tokenizer function will seperate each word from sentence, and then save it in form of list of strings\n",
    "=> ['Hello', 'my', 'name', 'is' , 'magician']\n",
    "\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How pre-processing of text / data is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How pre-processing of text/ data is done:\n",
    "\n",
    "german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "\n",
    "# Field function use list the tokenizer function made, Lower for making every letter lowercase,\n",
    "#                    init_token is for specifing start of sentence(<sos>) , eos_token for end of sentence (<eos>)\n",
    "\n",
    "english = Field(\n",
    "    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
    ")\n",
    "# Field function use list the tokenizer function made, Lower for making every letter lowercase,\n",
    "#                    init_token is for specifing start of sentence(<sos>) , eos_token for end of sentence (<eos>)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"), fields=(german, english)\n",
    ")\n",
    "\n",
    "## Spliting data with extension .de for german, .en for english, in field\n",
    "##               same order as for exts, use for calling Field functions for each language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary for each language: \n",
    "\n",
    "# For german vocabulary:\n",
    "\n",
    "german.build_vocab(train_data, max_size=10000, min_freq=2)  \n",
    "#               min_freq = 2 is used so that, word should have to be repeated atleast 2 times,\n",
    "#                           then only it will be added to vocabulary\n",
    "\n",
    "# For English Vocabulary:\n",
    "\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2) \n",
    "#               min_freq = 2 is used so that, word should have to be repeated atleast 2 times,\n",
    "#                            then only it will be added to vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first LSTM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "                       # input_size= size of vocab.(for german lang) ,\n",
    "                       #   embedding_size so that each word is mapped to some D-space\n",
    "                       #          num_layer= no. of layers in Encoder\n",
    "        super(Encoder, self).__init__()\n",
    "                  #  The super() function in Python makes class inheritance more manageable and extensible. The function returns a temporary object that allows reference to a parent class by the keyword super.\n",
    "                  # The super() function has two major use cases:\n",
    "                  #  To avoid the usage of the super (parent) class explicitly.\n",
    "                  #  To enable multiple inheritances​.    \n",
    "        self.dropout = nn.Dropout(p)    # p= how many nodes to drop.\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)    # Embedding of some input_size mapped to embedding_size\n",
    "\n",
    "                       # first we run input on embedding, then we run RNN on those embeddings\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)  \n",
    "                            #  input of LSTM is embedding_size, ouyputting hidden_size, dropout is key argument.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # sending an input of a long vector of the indexes of words in vocab through tokenize func()\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size) \n",
    "        #      N is shape of X, so that each word is mapped to embedding , by adding embedding size, we added another Dimension to our tensor.\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "            # imp. to us are hidden and cell.\n",
    "        return hidden, cell\n",
    "\n",
    "# Second LSTM\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        \n",
    "         # input_size= size of vocab.(for English lang) ,\n",
    "         #   embedding_size so that each word is mapped to some D-space\n",
    "         #   num_layer= no. of layers in Decoder\n",
    "         # output_size =  same as input_size, but gives value of vector for each word in vocab.\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)     # p= how many nodes to drop.\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)    # Embedding of some input_size mapped to embedding_size\n",
    "            # first we run input on embedding, then we run RNN on those embeddings\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "                 #  input of LSTM is embedding_size, ouyputting hidden_size, dropout is key argument.\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "                 #fc= fully connected : linear for hiden_size, output_size\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "            # sending an input of a long vector of the indexes of words in vocab through tokenize func()\n",
    "\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0)  #adding 1 D.\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "# for combining encoder and decoder \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "                            # teacher_force_ratio  is used so that model will be fed with\n",
    "                            # 50% correct sentence/words and 50 incorrect.\n",
    "                            # so that it doesn't create problem when we fed test data to machine.\n",
    "                            \n",
    "        batch_size = source.shape[1]  #second dimension of source shape\n",
    "        \n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "                # Predict one word at a time, and for each word we predict\n",
    "                # we gonna do it for entire batch , and then every prediction will\n",
    "                # be sort of that vector of entire vocab size.\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're ready to define everything we need for training our Seq2Seq model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # if gpu is available , then choose that , otherwise cpu.\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For saving the loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")           # Bucket Iterator: Defines an iterator that batches examples of similar lengths together.\n",
    "            # Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. \n",
    "            # lambda func() in sort_key is used to prioritize examples of similar lenght\n",
    "            # so that to minimise number of paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all variables sent in function are declared in cell above named hyperparameter\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "\n",
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import sys\n",
    "\n",
    "\n",
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "    # print(sentence)\n",
    "\n",
    "    # sys.exit()\n",
    "\n",
    "    # Load german tokenizer\n",
    "    spacy_ger = spacy.load(\"de\")\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # print(tokens)\n",
    "\n",
    "    # sys.exit()\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]\n",
    "\n",
    "\n",
    "def bleu(data, model, german, english, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in data:\n",
    "        src = vars(example)[\"src\"]\n",
    "        trg = vars(example)[\"trg\"]\n",
    "\n",
    "        prediction = translate_sentence(model, src, german, english, device)\n",
    "        prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['toilets', 'rollerskating', 'alcoholic', 'alcoholic', 'images', 'times', 'vendors', 'times', 'do', 'do', 'observe', 'waters', 'waters', 'do', 'skyscraper', 'firefighters', 'formations', 'fluffy', 'slowly', 'slowly', 'where', 'ascends', 'costumes', 'hall', 'arms', 'visible', 'visible', 'hung', 'hung', 'paddlers', 'slowly', 'descends', 'diving', 'diving', 'space', 'space', 'trolley', 'trolley', 'friends', 'flotation', 'low', 'diving', 'enthusiastically', 'enthusiastically', 'enthusiastically', 'insect', 'insect', 'dogs', 'helmet', 'through']\n",
      "[Epoch 1 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boy', 'in', 'a', 'red', 'shirt', 'is', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "[Epoch 2 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'a', '<unk>', '<unk>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n",
      "[Epoch 3 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', '<unk>', 'with', 'with', 'a', '<unk>', '<unk>', 'is', '<unk>', 'the', '<unk>', 'of', 'a', '<unk>', '.', '<eos>']\n",
      "[Epoch 4 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'bicyclist', 'with', 'with', 'a', 'number', '<unk>', 'is', 'being', 'by', 'a', 'large', '.', '.', '<eos>']\n",
      "[Epoch 5 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'a', 'number', '<unk>', 'is', 'being', 'pulled', 'by', 'a', 'large', '.', '<eos>']\n",
      "[Epoch 6 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'police', 'officer', 'with', 'wings', 'is', 'pulled', 'by', 'a', 'large', '<unk>', 'of', 'a', '.', '<eos>']\n",
      "[Epoch 7 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', '<unk>', 'pulled', 'by', 'a', 'large', '<unk>', 'of', 'a', 'large', '.', '.', '<eos>']\n",
      "[Epoch 8 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', 'large', 'pulled', 'from', 'a', 'large', 'large', 'large', 'large', 'large', '.', '.', '<eos>']\n",
      "[Epoch 9 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'passengers', 'pulled', 'by', 'a', 'large', 'large', 'large', 'large', 'large', 'large', '.', '<eos>']\n",
      "[Epoch 10 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'many', 'men', 'pulled', 'pulled', 'from', 'a', 'large', 'full', 'of', 'large', '.', '.', '<eos>']\n",
      "[Epoch 11 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'by', 'large', 'large', '.', '.', '<eos>']\n",
      "[Epoch 12 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', '.', '<eos>']\n",
      "[Epoch 13 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', '.', '<eos>']\n",
      "[Epoch 14 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'being', 'pulled', 'by', 'a', 'large', 'large', 'by', 'a', 'large', '.', '<eos>']\n",
      "[Epoch 15 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'from', 'a', 'large', '.', '<eos>']\n",
      "[Epoch 16 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'surrounded', 'by', 'large', 'foliage', '.', '<eos>']\n",
      "[Epoch 17 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', '<unk>', 'by', 'a', '.', '<eos>']\n",
      "[Epoch 18 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'to', 'pulled', 'by', 'a', 'large', 'by', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 19 / 20]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'being', 'pulled', 'by', 'a', 'large', 'by', 'a', 'large', 'horses', '.', '<eos>']\n",
      "Bleu score 18.17\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "            # Get input and targets and get to cuda\n",
    "            inp_data = batch.src.to(device)\n",
    "            target = batch.trg.to(device)\n",
    "\n",
    "            # Forward prop\n",
    "            output = model(inp_data, target)\n",
    "\n",
    "            # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "            # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "            # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "            # way that we have output_words * batch_size that we want to send in into\n",
    "            # our cost function, so we need to do some reshapin. While we're at it\n",
    "            # Let's also remove the start token while we're at it\n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            target = target[1:].reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Back prop\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "            # within a healthy range\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "            # Gradient descent step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Plot to tensorboard\n",
    "            writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "            step += 100\n",
    "\n",
    "\n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 21.06\n"
     ]
    }
   ],
   "source": [
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model runs successfully.\n",
    "You can see the loss plot by running the command:  \n",
    "tensorboard --logdir runs\n",
    "\n",
    "in the command prompt or shell,\n",
    "this will make a server with port 6006 (for tensorflow.) and you can see the graphs of plot there nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
